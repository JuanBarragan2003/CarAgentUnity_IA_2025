# CarAgentUnity_IA_2025

# ğŸš— CarAgentUnity_IA_2025

Este repositorio contiene un agente inteligente de conducciÃ³n autÃ³noma desarrollado en **Unity** con soporte para **ML-Agents** y algoritmos personalizados como **PPO** de la herramienta de la MLagents, **SARSA** y **DQL**. El agente aprende a recorrer un circuito cerrado pasando por checkpoints ordenados, evitando obstÃ¡culos y maximizando la recompensa.
---
## ğŸ¯ Objetivos del Proyecto

- Desarrollar un entorno de simulaciÃ³n de conducciÃ³n en Unity.
- Entrenar un agente usando algoritmos de aprendizaje por refuerzo.
- Implementar y comparar modelos con:
  - **ML-Agents (PPO)**
  - **SARSA** (con aprocimaciones mediante Fourier)
  - **DQL clÃ¡sico**
- Evaluar el comportamiento con mÃ©tricas detalladas y grÃ¡ficas.

---

## ğŸ“ Estructura del Repositorio

```plaintext
CarAgentUnity_IA_2025/
â”‚
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ CarAgent.cs           # Script del agente para PPO 
â”‚   â”œâ”€â”€ EnvironmentManager.cs # Reinicio y lÃ³gica del entorno
â”‚   â”œâ”€â”€ HUDManager.cs         # HUD con mÃ©tricas visuales
â”‚   â””â”€â”€ RaySensorGizmo.cs     # VisualizaciÃ³n de raycasts
â”‚   â”œâ”€â”€ Prefacs               # Escena principal del entorno
â”‚   â”œâ”€â”€ train_sarsa.py         # Entrenamiento SARSA personalizado
â”‚   â”œâ”€â”€ train_dql.py            # Entrenamiento DQL clÃ¡sico
â”‚   â”œâ”€â”€ sarsa.py        # CodificaciÃ³n de Fourier para observaciones
â”‚  
â”‚
â”œâ”€â”€ Results_SARSA/
â”‚   â”œâ”€â”€ episodic_rewards.csv
â”‚   â”œâ”€â”€ reward_vs_steps.png
â”‚   â””â”€â”€ checkpoints_plot.png
â”‚
â”œâ”€â”€ Results_PPO/
â”‚   â”œâ”€â”€ summaries/
â”‚   â”œâ”€â”€ plots/
â”‚   â””â”€â”€ training_logs/
â”‚   â”œâ”€â”€ config/
â”‚       â””â”€â”€ car_config.yaml   # ConfiguraciÃ³n de entrenamiento PPO

```

***

## ğŸ§ª MetodologÃ­a de Entrenamiento

El entrenamiento del agente fue realizado usando dos enfoques complementarios:

### ğŸ” 1. Entrenamiento con PPO (Proximal Policy Optimization)

Utilizamos **ML-Agents** de Unity para entrenar al agente directamente dentro del entorno 3D.

**Pasos realizados:**

1. Definimos el comportamiento del agente en `Agent.cs`, utilizando observaciones como raycasts, velocidad, y distancia al checkpoint.
2. Instalamos la herramienta MLagents de unity. 
3. Configuramos los parÃ¡metros de entrenamiento en el archivo `car_config.yaml`, incluyendo hiperparÃ¡metros como:
   - `learning_rate = 0.0003`
   - `buffer_size = 10240`
   - `batch_size = 1024`
   - `gamma = 0.99`, entre otros.
4. Ejecutamos el entorno desde Unity y conectamos ML-Agents con:
   ```bash
   mlagents-learn config/car_config.yaml --run-id=ppo_run_1
 ```
 ```
***

## ğŸ”‚ Entrenamiento con SARSA

Implementamos un entrenamiento personalizado utilizando el algoritmo **SARSA (State-Action-Reward-State-Action)**, ideal para entornos con estados discretizados o codificados, como es el caso de este proyecto, donde usamos una base de funciones de **Fourier** para representar las observaciones continuas del entorno.

### ğŸ“‹ DescripciÃ³n del algoritmo

SARSA actualiza su tabla Q en funciÃ³n de la polÃ­tica actual. A diferencia de Q-Learning, aprende en tiempo real de la secuencia completa (estado, acciÃ³n, recompensa, siguiente estado, siguiente acciÃ³n), lo que lo hace mÃ¡s estable en entornos no deterministas como Unity.

### ğŸ§© Observaciones utilizadas

- Distancias a obstÃ¡culos desde sensores Raycast.
- Velocidad del vehÃ­culo.
- Distancia al siguiente checkpoint.
- NÃºmero de checkpoint alcanzado.

Estas observaciones se codifican mediante Fourier para convertirlas en una representaciÃ³n mÃ¡s compacta y generalizable.

### âš™ï¸ Entrenamiento paso a paso

1. AsegÃºrate de tener Unity corriendo en modo "External Communicator".
2. Ejecuta el entrenamiento con:
   ```bash
   python  scripts/sarsa.py --episodes 5000 --steps 3000 --gamma 0.99 --alpha 0.001 --epsilon 0.999




